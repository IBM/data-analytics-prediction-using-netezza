{"cells":[{"metadata":{},"cell_type":"markdown","source":["# Data science with Python and Netezza Performance Server\n","\n","After the release of _Netezza Performance Server on Cloud Pak for Data_ (Netezza on Cloud) and _Netezza Performance Server on Cloud Pak for Data System_ (Netezza on prem) the next step in the modernization of Netezza is the ability to couple Netezza's in database analytics with Python's data science and visualization capabilities. \n","\n","With the release of [nzpy](https://pypi.org/project/nzpy/) you can connect to and work with Netezza on Cloud and Netezza on prem from any OS that support python. It even unlocks the ability to stream data directly to and from the Netezza system without any other software or drivers.\n","\n","Lets look at two parts \n","\n","- First part will cover programming basics with nzpy\n","- The second will put everything together and actually go through a simple data science use case\n","\n","\n","## Nzpy programming basics\n","\n","`nzpy` is a pure python driver implementation of the [Python DBAPI 2.0](https://www.python.org/dev/peps/pep-0249/DBApi) and so apart from a `pip install` there is no other pre-requisite."]},{"metadata":{},"cell_type":"markdown","source":["## Install pre-requisites"]},{"metadata":{},"cell_type":"code","source":["!pip install nzpy"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Connecting to the database \n","\n","The connection only requires the hostname of the Netezza "]},{"metadata":{},"cell_type":"code","source":["# Setup connection and use the credentials from the connection. Replace the following values before you start\n","\n","# from project_lib import Project\n","# project = Project.access()\n","# NPS_credentials = project.get_connection(name=\"NPS\")\n","\n","## OR\n","\n","username=\"<username>\"\n","password=\"<password>\"\n","host=\"<hostname or ip>\"\n","database=\"system\"\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["import nzpy\n","import os\n","\n","db = 'NZPY_TEST'\n","con = nzpy.connect(user=username, password=password, host=host,\n","                   database=database, port=5480)\n","selectQuery = f\"select 1 from _v_database where database = '{db}'\"\n","createQuery = f\"create database {db}\"\n","## Make sure the database exists, if not create one\n","with con.cursor() as cur:\n","    cur.execute(selectQuery)\n","    r = cur.fetchone();\n","    if r is None:\n","        cur.execute(createQuery)\n","        \n","# Now connect using the new database.        \n","con = nzpy.connect(user=username, password=password, host=host,\n","                   database=db, port=5480)\n","cursor=con.cursor()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# fetch teh databases to make sure the database is created\n","\n","cursor.execute(\"select * from _v_database limit 10\")\n","row = cursor.fetchone()\n","while row: \n","    print(row)\n","    row = cursor.fetchone()\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["The `con` is a Python DBAPI [Connection](https://www.python.org/dev/peps/pep-0249/#connection-objects) that lets one interact with the Netezza database using its [Cursor](https://www.python.org/dev/peps/pep-0249/#cursor-objects) \n","\n","### Working with Netezza database \n","\n","The `con` and `con.cursor()` objects can be used to interact with the Netezza database. [pandas.Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) can be used for quick result visualization in addition to its main purpose as the foundation of data science.  For example"]},{"metadata":{},"cell_type":"code","source":["import pandas as pd\n","result = pd.read_sql_query(\"select database, owner, createdate from _v_database where owner like 'ADMIN'\", con)\n","# make result column names human friendly\n","result.columns = [c.decode().lower() for c in result.columns]\n","result"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["`nzpy` also supports streaming data loads and unloads. There are quite a few variations we can do here. \n","\n","#### Load or unload regular flat files "]},{"metadata":{},"cell_type":"markdown","source":["Before that lets setup a database so that we can create tables programmatically. From the NPS console, create a table called `nzpytest`."]},{"metadata":{},"cell_type":"code","source":["cursor.execute(\"drop table nzpy_test..ORDERS if exists\")\n","\n","cursor.execute('''\n","CREATE TABLE ORDERS  ( O_ORDERKEY       INTEGER NOT NULL PRIMARY KEY,\n","                           O_CUSTKEY        INTEGER NOT NULL,\n","                           O_ORDERSTATUS    CHAR(1) NOT NULL,\n","                           O_TOTALPRICE     DECIMAL(15,2) NOT NULL,\n","                           O_ORDERDATE      DATE NOT NULL,\n","                           O_ORDERPRIORITY  CHAR(15) NOT NULL,\n","                           O_CLERK          CHAR(15) NOT NULL,\n","                           O_SHIPPRIORITY   INTEGER NOT NULL,\n","                           O_COMMENT        VARCHAR(79) NOT NULL)\n","                           ORGANIZE ON NONE;\n","''')\n","print(\"Table ORDERS created\")        "],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Loading data from local files to NPS uses streaming transient external table with `REMOTESOURCE 'python'` option as shown below:"]},{"metadata":{},"cell_type":"code","source":["# Make sure to change the path where your sales.csv is located. \n","#Relative path is in the repository you cloned.\n","with con.cursor() as cursor:\n","    cursor.execute('''\n","        insert into nzpy_test..ORDERS\n","            select * from external '/project_data/data_asset/orders.tbl'\n","                using (\n","                    delim '|' \n","                    remotesource 'odbc'\n","                    skiprows 1)''')\n","    print(f\"{cursor.rowcount} rows inserted\")\n","    # cursor.rowcount will report the number of rows loaded"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["The same works in reverse to load local files "]},{"metadata":{"scrolled":true},"cell_type":"code","source":["with con.cursor() as cursor:\n","    cursor.execute('''\n","      create external table '/tmp/orders.tbl'\n","        using (\n","            delim '|' \n","            remotesource 'odbc'\n","            includeheader yes\n","        ) as select * from nzpy_test..orders limit 20''')    \n","pd.read_csv('/tmp/orders.csv', delimiter='|')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["#### Loading from other data sources\n","\n","Data sources, like external servers, github etc can be fit into this model by streaming data from the source through the python application itself. The python using `nzpy` can read data from external sources and connect that directly to `nzpy` pipe via a named pipe. \n","\n","_(Note: The named pipe method below works seamlessly on Linux and Mac. For windows win32pipe module can be used to achieve the same)_\n","\n","Creating a pipe for all data streaming loads lets one have a thread that can push data to this pipe. The other end is connected to Netezza via an external table.\n"]},{"metadata":{},"cell_type":"code","source":["import pathlib\n","datapipe = pathlib.Path(\"/tmp/datapipe\")\n","\n","def create_datapipe():\n","    global datapipe\n","    if datapipe.exists():\n","        print(f\"Cleaning prior {datapipe}\")\n","        datapipe.unlink()\n","    \n","    print(f\"Initializing fifo\", end='..')\n","    os.mkfifo(datapipe)\n","    print('Done')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["The `requests` module coupled with `shutil.copyobject` can be used to actually stream rather than spool data. Here's an example of streaming data that is obtained as gzip'd from github"]},{"metadata":{},"cell_type":"code","source":["import requests, shutil, subprocess, gzip\n","\n","def load_published_dataset(ds):\n","    with open(datapipe, \"wb\") as pipe:\n","        with requests.get(ds, stream=True) as r:\n","            with gzip.GzipFile(fileobj=r.raw) as unzip:\n","                shutil.copyfileobj(unzip, pipe)\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Connecting the two together, and creating an external table over the named pipe will like this - \n","\n","_(Note: the pipe is put in a separate thread so that nzpy doesn't block the data stream)_\n","\n","The `Cursor.rowcount` attribute will report the number of rows loaded"]},{"metadata":{},"cell_type":"code","source":["cursor=con.cursor()\n","cursor.execute('drop table nzpy_test..CARS if exists')\n","cursor.execute('''\n","                    CREATE TABLE nzpy_test..CARS  ( MPG       NUMERIC(5,2),\n","                           CYLINDERS        INTEGER,\n","                           ENGINE    NUMERIC(5,2),\n","                           HORSEPOWER     INTEGER,\n","                           WEIGHT      INTEGER,\n","                           ACCELERATION  NUMERIC(5,2),\n","                           YEAR          INTEGER,\n","                           ORIGIN   VARCHAR(10),\n","                           NAME        VARCHAR(40))\n","                           ORGANIZE ON NONE;\n","                '''\n","              )"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["import threading\n","\n","create_datapipe()\n","if datapipe.exists() and datapipe.is_fifo():\n","    print(\"Pipe ready\")\n","\n","source = 'https://raw.githubusercontent.com/ibm-watson-data-lab/open-data/master/cars/cars.csv'\n","streamer = threading.Thread(target=load_published_dataset, args=(source,))\n","streamer.start()\n","\n","with con.cursor() as cursor:                            \n","    cursor.execute(f''' \n","        insert into nzpy_test..cars select * from external '{datapipe}' \n","            using (\n","                delim ','\n","                remotesource 'odbc'\n","                skiprows 1\n","            )''')\n","    print(f\"{cursor.rowcount} rows inserted\")\n","    \n","streamer.join()\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Doing it in revserse and specifying a file instead of named pipe will unload the data. `pandas.Dataframe` can be used to directly read the output of a query and then do further analytics on it. "]},{"metadata":{},"cell_type":"markdown","source":["### Data ingestion and ELT from object store\n","\n","NPS can load and unload data from object stores like Amazon S3 and IBM Cloud object store. This works by using Netezza External Tables to read from and write to object store.\n","\n","Lets take a look at a few examples; lets target an AWS S3 bucket. Prerequisites -\n","\n","- AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_REGION environment variables are set with the correct credentials\n","- BUCKET environment variable has the correct bucket name\n","\n","_Note:_ One can configure the ACLs for the bucket on AWS like [this](https://www.ibm.com/support/knowledgecenter/en/SSTNZ3/com.ibm.ips.doc/postgresql/admin/adm_nps_cloud_provisioning_prereq_aws.html) to balance security and need for NPS to read/write to it."]},{"metadata":{},"cell_type":"code","source":["import os\n","s = 'AWS_ACCESS_KEY_ID=<AWS ACCESS KEY ID> AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY> AWS_REGION=<AWS_REGION> BUCKET=<BUCKET_NAME>'\n","os.environ.update(dict(i.split('=') for i in s.split()))\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Setup the table schema based on the json mentioned above\n","schema = '''\n","    iso_code varchar (16),\n","    continent varchar(48),\n","    location varchar(48),\n","    covid_date date,\n","    total_cases numeric(32, 20),\n","    new_cases numeric(32, 20),\n","    new_cases_smoothed numeric(32, 20),\n","    total_deaths numeric(32, 20),\n","    new_deaths numeric(32, 20),\n","    new_deaths_smoothed numeric(32, 20),\n","    total_cases_per_million numeric(32, 20),\n","    new_cases_per_million numeric(32, 20),\n","    new_cases_smoothed_per_million numeric(32, 20),\n","    total_deaths_per_million numeric(32, 20),\n","    new_deaths_per_million numeric(32, 20),\n","    new_deaths_smoothed_per_million numeric(32, 20),\n","    new_tests numeric(32, 20),\n","    total_tests numeric(32, 20),\n","    total_tests_per_thousand numeric(32, 20),\n","    new_tests_per_thousand numeric(32, 20),\n","    new_tests_smoothed numeric(32, 20),\n","    new_tests_smoothed_per_thousand numeric(32, 20),\n","    tests_per_case numeric(32, 20),\n","    positive_rate numeric(32, 20),\n","    tests_units varchar(32),\n","    stringency_index numeric(32, 20),\n","    population numeric(32, 20),\n","    population_density numeric(32, 20),\n","    median_age numeric(32, 20),\n","    aged_65_older numeric(32, 20),\n","    aged_70_older numeric(32, 20),\n","    gdp_per_capita numeric(32, 20),\n","    extreme_poverty numeric(32, 20),\n","    cardiovasc_death_rate numeric(32, 20),\n","    diabetes_prevalence numeric(32, 20),\n","    female_smokers numeric(32, 20),\n","    male_smokers numeric(32, 20),\n","    handwashing_facilities numeric(32, 20),\n","    hospital_beds_per_thousand numeric(32, 20),\n","    life_expectancy numeric(32, 20),\n","    human_development_index numeric(32, 20)'''\n","\n","# Read data on the fly and lets see if all is working well. \n","df = pd.read_sql(f'''\n","    select unique(continent)\n","    from external 'owid-covid-data.csv' ({schema})\n","    using (\n","        remotesource 'S3' \n","        delim ','\n","        uniqueid 'covid' \n","        accesskeyid '{os.environ[\"AWS_ACCESS_KEY_ID\"]}'\n","        secretaccesskey '{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}'\n","        defaultregion '{os.environ[\"AWS_REGION\"]}'\n","        bucketurl '{os.environ[\"BUCKET\"]}'\n","        skiprows 1\n","    ) where continent is not null and continent != '' ''', con)\n","df.columns = [c.decode().lower() for c in df.columns]\n","df"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Load data from S3 to Netezza"]},{"metadata":{},"cell_type":"code","source":["cursor=con.cursor()\n","table = 'covid'\n","with con.cursor() as cur:\n","    # drop any old table\n","    r = cur.execute(f\"select 1 from _v_table where tablename = '{table}'\")\n","    if r.fetchall():\n","        cur.execute(f'drop table {table}')\n","        \n","    # create a table to load data\n","    cur.execute(f'create table {table} ({schema})')\n","    print(f\"Table {table} created\")\n","    \n","    # load data from object store\n","    cur.execute(f'''\n","    insert into {table} \n","        select * from external 'owid-covid-data.csv' ({schema})\n","        using (\n","            remotesource 'S3' \n","            delim ','\n","            uniqueid 'covid' \n","            accesskeyid '{os.environ[\"AWS_ACCESS_KEY_ID\"]}'\n","            secretaccesskey '{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}'\n","            defaultregion '{os.environ[\"AWS_REGION\"]}'\n","            bucketurl '{os.environ[\"BUCKET\"]}'\n","            skiprows 1\n","        )''')\n","    print(f\"{cur.rowcount} Rows loaded\")"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Get a week over week trend\n","df = pd.read_sql_query('''\n","    select continent,\n","            this_week(covid_date) as wk,\n","            max(new_cases) as total \n","    from covid \n","    where \n","        continent is not null and\n","        continent != ''\n","    group by wk, continent\n","    order by wk, continent\n","    ''', con,\n","    parse_dates = {b'WK': '%Y-%m-%d'})\n","df.columns = [c.decode().lower() for c in df.columns]\n","df.total = df.total.astype(float)\n","df.head()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Lets visualize the same\n","from mizani.formatters import date_format\n","from plotnine import *\n","\n","( ggplot(df, aes(x='wk', y='total', color='continent')) + geom_line() + geom_point() + \n","  labs(y = \"Total cases\", x = \"Week\") + facet_wrap('continent') + \n","   scale_x_datetime(labels=date_format('%b %-d')) +\n","   theme(axis_text_x=element_text(rotation=60, hjust=1))\n",")"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## A real life example\n","\n","In this example lets use Python and Netezza Performance Server, to load and analyze the data on [Australian temperatures and rainfall published publically](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-07/readme.md) \n","\n","The best practices are \n","\n","- Load data into Netezza \n","- Do as most of the filtering, transformation and analytics in database\n","- Do the last step of visualizing and final analytics by extracting the smaller result of the above step in Python\n","\n","### Step 1 - Dataset\n","\n","Lets look at the dataset. For the first go around, setup the tables. The data here represents the [Australian weather station temperature and rainfall data as of Jan 1 2020](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-07/readme.md) \n","\n","There are two tables \n","\n","**temperature.csv**\n","\n","|column    |type     |description |\n","|:-----------|:---------|:-----------|\n","|city_name   |VARCHAR(20) | City Name|\n","|date        |DATE    | Date |\n","|temperature |NUMERIC(8,3)    | Temperature in Celsius |\n","|temp_type   |VARCHAR(10) | Temperature type (min/max daily) |\n","|site_name   |VARCHAR(30) | Actual site/weather station|\n","\n","**rainfall.csv**\n","\n","|column     |type     |description |\n","|:------------|:---------|:-----------|\n","|station_code |INT | Station Code |\n","|city_name    |VARCHAR(20) | City Name |\n","|year         |INT    | Year |\n","|month        |INT  | Month |\n","|day          |INT | Day |\n","|rainfall     |NUMERIC(8,3)    | rainfall in millimeters|\n","|period       |INT    | how many days was it collected across |\n","|quality      |VARCHAR(5) | Certified quality or not |\n","|lat          |NUMERIC(5,2)    | latitude |\n","|long         |NUMERIC(5,2)    | longitude |\n","|station_name |VARCHAR(30) | Station Name |\n","\n","\n","### Step 2 - Setup the tables"]},{"metadata":{},"cell_type":"code","source":["with con.cursor() as cursor:\n","    # find which of the tables already exist\n","    existing = { table[0] for table in cursor.execute(\"select lower(tablename) from _v_table where lower(tablename) in ('rainfall', 'temperature')\").fetchall() }\n","    if 'temperature' not in existing:\n","        # create only if they don't\n","        cursor.execute('''\n","        create table nzpy_test..temperature (\n","            city_name     VARCHAR(20),\n","            date          DATE,\n","            temperature   NUMERIC(8,3),\n","            temp_type     VARCHAR(10),\n","            site_name     VARCHAR(30)\n","        ) distribute on (city_name)\n","        ''')\n","        print(\"Table temperature created\")\n","        \n","    if 'rainfall' not in existing:\n","        cursor.execute('''\n","        create table nzpy_test..rainfall (\n","            station_code  INT,\n","            city_name     VARCHAR(20),\n","            year          INT,\n","            month         INT,\n","            day           INT,\n","            rainfall      NUMERIC(8,3),\n","            period        INT,\n","            quality       VARCHAR(5),\n","            lat           NUMERIC(5,2),\n","            long          NUMERIC(5,2),\n","            station_name  VARCHAR(100)\n","        ) distribute on (city_name, station_code)\n","        ''')\n","print(\"Table rainfall created\")"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Now stream and load both tables "]},{"metadata":{},"cell_type":"code","source":["create_datapipe()\n","\n","rainfall = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-07/rainfall.csv\"\n","loader = threading.Thread(target=load_published_dataset, args=(rainfall,))\n","loader.start()\n","\n","with con.cursor() as cursor:\n","    print(\"Loading data\", end=\".. \")\n","    result = cursor.execute(f'''INSERT INTO nzpy_test..rainfall SELECT * FROM EXTERNAL '{datapipe}'\n","                                USING (\n","                                    DELIMITER ','\n","                                    REMOTESOURCE 'ODBC'\n","                                    NULLVALUE 'NA'\n","                                    SKIPROWS 1)''')\n","    print(f\"{cursor.rowcount} rows inserted\")\n","    \n","loader.join()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["create_datapipe()\n","temperature = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-07/temperature.csv\"\n","\n","loader = threading.Thread(target=load_published_dataset, args=(temperature,))\n","loader.start()\n","\n","with con.cursor() as cursor:\n","    print(\"Loading data\", end=\".. \")\n","    result = cursor.execute(f'''INSERT INTO nzpy_test..temperature SELECT * FROM EXTERNAL '{datapipe}'\n","                                USING (\n","                                    DELIMITER ','\n","                                    REMOTESOURCE 'ODBC'\n","                                    NULLVALUE 'NA'\n","                                    SKIPROWS 1)''')\n","    print(f\"{cursor.rowcount} rows inserted\")\n","    \n","loader.join()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Step - 3 Analyze the data\n","\n","First analyze the data in database by grouping the temperatures across months and decades to reduce the dataset. After that visualization and further analytics can be done easily in python"]},{"metadata":{},"cell_type":"code","source":["df = pd.read_sql('''\n","    select extract(decade from date) * 10 as decade,\n","           extract(month from date) as month,\n","           city_name,\n","           avg(temperature) as avg\n","        from nzpy_test..temperature\n","        where city_name in ('SYDNEY', 'MELBOURNE')\n","        group by month, decade, city_name\n","        order by month\n"," ''', con)\n","\n","df.columns = [c.decode().lower() for c in df.columns]\n","df.decade = df.decade.astype(int)\n","df.avg = df.avg.astype(float)\n","df.month = df.month.astype(int)\n","df"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Now we can combine `DataFrame` with `ggplot` too see how average temperatures for two cities across the year stack up across all decades for the last 100 years"]},{"metadata":{"scrolled":false},"cell_type":"code","source":["import matplotlib, calendar\n","from plotnine import *\n","%matplotlib inline\n","\n","( ggplot(df, aes(x='month', y='avg', group='decade', color='decade')) + geom_line() + geom_point() + \n","  labs(y = \"Average Temperature (°C)\", x = \"Month\") + facet_wrap('city_name') +\n","   scale_color_gradient(low=\"blue\", high=\"red\") + \n","   scale_x_discrete(labels=list(calendar.month_abbr[1:]), limits=range(12)) + \n","   theme(axis_text_x=element_text(rotation=60, hjust=1))\n",")"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Next Steps"]},{"metadata":{},"cell_type":"markdown","source":["So far we have interacted with Netezza with DDL and DML sql scripts from varous sources such as CSV files, files stores in github, object store from S3.\n","\n","In the next notebook we will be learning more about the analytical and machine learning capabilities of Netezza Performance Server."]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.6","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}